# -*- coding: utf-8 -*-
"""Liver_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gom17Bz-cWZmigCA1Ae3JSk_hMGZbbY6
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing required Libraries:
import numpy as np
import pickle
import os
import seaborn as sns
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score
import warnings; warnings.simplefilter('ignore')
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pickle
import os
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import SelectKBest, RFECV
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import LogisticRegression
from xgboost.sklearn import XGBClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sn
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
import warnings; warnings.simplefilter('ignore')
# %matplotlib inline

"""**Upload the datasets using the pandas**"""

ldp_data=pd.read_csv("/content/Liver_data.csv")

#reading the fisrt five dataset
ldp_data.head()

#reading the last five dataset
ldp_data.tail()

#shape of the datasets
ldp_data.shape

#columns in the dataset
ldp_data.columns

#datasets analysis
ldp_data.describe()

"""# Data Cleaning"""

#Checking for the empty data in the dataset
ldp_data.isnull().sum()

# Mean & Median of "Albumin_and_Globulin_Ratio" feature,these are null dataset:
print(ldp_data['Albumin_and_Globulin_Ratio'].median())
print(ldp_data['Albumin_and_Globulin_Ratio'].mean())

#replace the empty data with the mean and meadia
ldp_data['Albumin_and_Globulin_Ratio'] = ldp_data['Albumin_and_Globulin_Ratio'].fillna(ldp_data['Albumin_and_Globulin_Ratio'].median())

#Checking for the empty data in the dataset
ldp_data.isnull().sum()

ldp_data.dtypes

ldp_data.head()

# Create a label encoder object
label_encoder = LabelEncoder()

# Apply label encoding to the "gender" column
ldp_data['Gender'] = label_encoder.fit_transform(ldp_data['Gender'])

# Gender feature:
print("Total Female :", ldp_data['Gender'].value_counts()[0])
print("Total Male :", ldp_data['Gender'].value_counts()[1])

# Visualization:
sns.countplot(ldp_data['Gender'])
plt.show()

# Target feature:
print("Liver Disease Patients      :", ldp_data['Dataset'].value_counts()[1])
print("Non Liver Disease Patients  :", ldp_data['Dataset'].value_counts()[2])

# Visualization:
sns.countplot(ldp_data['Dataset'])
plt.show()

#CORRELATION OF THE DATA IN HEATMAP
ldp_data.corr()

plt.figure(figsize=(8,4))
sns.heatmap(ldp_data.corr(), annot=True, cmap='YlGnBu')
plt.show()

#assigning the target to variable y
#y is the output column
y=ldp_data.Dataset

#drop the variable y
#x is the input column
X=ldp_data.drop('Dataset', axis=1)

#split X and Y into train and test sets
X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=0 )

#number of observation in X_train,X_test, y_train,y_test
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""SCALLING DATA"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train[0,0:30]

y_train[0:30]

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

"""CHECKING ACCURACY WITH LR"""

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

accuracies = cross_val_score(estimator = classifier,
                             X = X_train,
                             y = y_train,
                             cv = 10,
                             n_jobs = -1)
accuracies.mean()

"""CONFUSION MATRIX FOR y-test and y prediction"""

cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=['Non-disease', 'Disease']))

df_cm = pd.DataFrame(cm, range(2), range(2))
sns.heatmap(df_cm, annot=True,fmt='g',cmap ='Blues')

"""FEATURE **SELECTION**"""

# find best scored 5 features
select_feature = SelectKBest(k=10).fit(X_train, y_train)

#create the object of each algorithm we used
model1=KNeighborsClassifier(n_neighbors=5)
model2=LogisticRegression()
model3=DecisionTreeClassifier()
model4=RandomForestClassifier(n_estimators=500)

#Train the model using fit
model1.fit(X_train,y_train)
model2.fit(X_train,y_train)
model3.fit(X_train,y_train)
model4.fit(X_train,y_train)

#predict the out from the trained model
y_pred1=model1.predict(X_test)
y_pred2=model2.predict(X_test)
y_pred3=model3.predict(X_test)
y_pred4=model4.predict(X_test)

#calculated the accuracy of all the algorithms used
acc=accuracy_score(y_test,y_pred1, normalize=True)*float(100)
print("Accuracy of KNN is {:.2f}%".format(acc))
acc=accuracy_score(y_test,y_pred2, normalize=True)*float(100)
print("Accuracy of LR is {:.2f}%".format(acc))
acc=accuracy_score(y_test,y_pred3, normalize=True)*float(100)
print("Accuracy of DT is {:.2f}%".format(acc))
acc=accuracy_score(y_test,y_pred4, normalize=True)*float(100)
print("Accuracy of RT is {:.2f}%".format(acc))

F1=f1_score(y_test,y_pred1)
print("F1_Score of KNN is {:.2f}%".format(acc))
F1=f1_score(y_test,y_pred2)
print("F1_Score of LR is {:.2f}%".format(acc))
F1=f1_score(y_test,y_pred3)
print("F1_Score of DT is {:.2f}%".format(acc))
F1=f1_score(y_test,y_pred4)
print("F1_Score of RT is {:.2f}%".format(acc))

pre=precision_score(y_test,y_pred1)
print("Precision_score of KNN is {:.2f}%".format(acc))
pre=precision_score(y_test,y_pred2)
print("Precision_score of LR is {:.2f}%".format(acc))
pre=precision_score(y_test,y_pred3)
print("Precision_score of DT is {:.2f}%".format(acc))
pre=precision_score(y_test,y_pred4)
print("Precision_score of RT is {:.2f}%".format(acc))

re= recall_score(y_test,y_pred1)
print("Recall_score of KNN is {:.2f}%".format(acc))
re= recall_score(y_test,y_pred2)
print("Recall_score of KNN is {:.2f}%".format(acc))
re= recall_score(y_test,y_pred3)
print("Recall_score of KNN is {:.2f}%".format(acc))
re= recall_score(y_test,y_pred4)
print("Recall_score of KNN is {:.2f}%".format(acc))

"""CROSS VALIDATION"""



#convert the final model into pickle to be used in frontend development
#save the model
joblib.dump(model4,'final_ldp_model.pk1')

#load the modelfrom the file
final_model=joblib.load('final_ldp_model.pk1')
pred=final_model.predict(X_test)

#check the accuracy again
acc=accuracy_score(y_test,pred, normalize=True)*float(100)
print("Final ldp model accuracy is {:.2f}%".format(acc))

F1=f1_score(y_test,pred)
print("Final ldp model F1_score is {:.2f}%".format(acc))